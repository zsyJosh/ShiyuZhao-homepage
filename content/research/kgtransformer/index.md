---
title: "Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries"
summary: A well-functioning pretrain-finetune paradigm on knowledge graph with great generalizability and interpretability. It formulates complex logical queries as masked predictions on graph patterns and introduces a two-stage masked pre-training strategy.  It also proposes a KG triple transformation method to enable transformer to handle KG elegantly and a mechanism to unify different tasks of knowledge graph problems. 
tags:
  - Natural Language Processing, Graph Representation Learning
date: '2016-04-27T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: Overview of the kgtransformer for complex query answering
  focal_point: Right

url_code: 'https://github.com/THUDM/kgTransformer'
url_pdf: ''
url_slides: ''
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: example
---

